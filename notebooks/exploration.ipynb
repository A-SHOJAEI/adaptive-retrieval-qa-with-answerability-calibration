{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Retrieval QA with Answerability Calibration - Exploration\n",
    "\n",
    "This notebook explores the adaptive retrieval question answering system that learns when to abstain from answering by combining MS MARCO passage retrieval with SQuAD 2.0's unanswerable question detection.\n",
    "\n",
    "## Key Innovation\n",
    "\n",
    "The system uses a novel confidence calibration approach that jointly models:\n",
    "- Retrieval relevance scores\n",
    "- Answer extraction confidence \n",
    "- Question characteristics\n",
    "\n",
    "This addresses the critical production problem of LLM hallucination when relevant information is unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path().parent / \"src\"))\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Project imports\n",
    "from adaptive_retrieval_qa_with_answerability_calibration.utils.config import Config\n",
    "from adaptive_retrieval_qa_with_answerability_calibration.data.loader import DatasetLoader\n",
    "from adaptive_retrieval_qa_with_answerability_calibration.data.preprocessing import DataPreprocessor\n",
    "from adaptive_retrieval_qa_with_answerability_calibration.models.model import AdaptiveRetrievalQAModel\n",
    "from adaptive_retrieval_qa_with_answerability_calibration.evaluation.metrics import AnswerabilityCalibrationMetrics\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration and Data Loading\n",
    "\n",
    "Let's start by loading the configuration and exploring the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = Config(\"../configs/default.yaml\")\n",
    "\n",
    "# Override for exploration (smaller datasets)\n",
    "config.set('data.train_size', 1000)\n",
    "config.set('data.val_size', 200)\n",
    "config.set('infrastructure.device', 'cpu')\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"- Device: {config.get('infrastructure.device')}\")\n",
    "print(f\"- Training size: {config.get('data.train_size')}\")\n",
    "print(f\"- Validation size: {config.get('data.val_size')}\")\n",
    "print(f\"- Retrieval top-k: {config.get('model.retrieval_top_k')}\")\n",
    "print(f\"- Confidence threshold: {config.get('model.confidence_threshold')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DatasetLoader(config)\n",
    "\n",
    "# Create sample dataset for exploration\n",
    "sample_data = {\n",
    "    'question': [\n",
    "        'What is the capital of France?',\n",
    "        'Who invented the telephone?',\n",
    "        'What is the speed of light?',\n",
    "        'When did World War II end?',\n",
    "        'What color is the sky?',\n",
    "        'How many legs does a spider have?',\n",
    "        'What is photosynthesis?',\n",
    "        'Who wrote Romeo and Juliet?',\n",
    "        'What is the largest planet?',\n",
    "        'When was Python created?',\n",
    "        'What is the meaning of life?',  # Potentially unanswerable\n",
    "        'How does quantum computing work?',  # Complex question\n",
    "    ],\n",
    "    'context': [\n",
    "        'Paris is the capital and largest city of France.',\n",
    "        'Alexander Graham Bell is credited with inventing the telephone in 1876.',\n",
    "        'The speed of light in vacuum is approximately 299,792,458 meters per second.',\n",
    "        'World War II ended in 1945 when Japan surrendered.',\n",
    "        'The sky appears blue due to Rayleigh scattering of light.',\n",
    "        'Spiders are arachnids with eight legs and two body segments.',\n",
    "        'Photosynthesis is the process by which plants convert sunlight into energy.',\n",
    "        'Romeo and Juliet was written by William Shakespeare.',\n",
    "        'Jupiter is the largest planet in our solar system.',\n",
    "        'Python was created by Guido van Rossum and first released in 1991.',\n",
    "        'Douglas Adams wrote that the answer is 42, but this is fictional.',\n",
    "        'Quantum computers use quantum mechanical phenomena like superposition.',\n",
    "    ],\n",
    "    'answers': [\n",
    "        {'text': ['Paris'], 'answer_start': [0]},\n",
    "        {'text': ['Alexander Graham Bell'], 'answer_start': [0]},\n",
    "        {'text': ['299,792,458 meters per second'], 'answer_start': [49]},\n",
    "        {'text': ['1945'], 'answer_start': [23]},\n",
    "        {'text': ['blue'], 'answer_start': [17]},\n",
    "        {'text': ['eight'], 'answer_start': [24]},\n",
    "        {'text': ['process by which plants convert sunlight into energy'], 'answer_start': [19]},\n",
    "        {'text': ['William Shakespeare'], 'answer_start': [36]},\n",
    "        {'text': ['Jupiter'], 'answer_start': [0]},\n",
    "        {'text': ['1991'], 'answer_start': [68]},\n",
    "        {'text': [], 'answer_start': []},  # Unanswerable\n",
    "        {'text': [], 'answer_start': []},  # Unanswerable\n",
    "    ],\n",
    "    'is_answerable': [True] * 10 + [False, False],\n",
    "    'source': ['exploration'] * 12,\n",
    "    'passage_id': [f'exp_{i}' for i in range(12)],\n",
    "    'relevance_score': [0.9, 0.8, 0.95, 0.85, 0.7, 0.9, 0.85, 0.8, 0.75, 0.6, 0.3, 0.4]\n",
    "}\n",
    "\n",
    "from datasets import Dataset\n",
    "sample_dataset = Dataset.from_dict(sample_data)\n",
    "\n",
    "print(f\"Created sample dataset with {len(sample_dataset)} examples\")\n",
    "print(f\"Answerable: {sum(sample_dataset['is_answerable'])}\")\n",
    "print(f\"Unanswerable: {len(sample_dataset) - sum(sample_dataset['is_answerable'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Analysis\n",
    "\n",
    "Let's analyze the characteristics of our dataset to understand the distribution of answerable vs unanswerable questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset statistics\n",
    "stats = data_loader.get_dataset_statistics(sample_dataset)\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 30)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset characteristics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Answerability distribution\n",
    "answerability_counts = [sum(sample_dataset['is_answerable']), \n",
    "                       len(sample_dataset) - sum(sample_dataset['is_answerable'])]\n",
    "axes[0, 0].pie(answerability_counts, labels=['Answerable', 'Unanswerable'], \n",
    "               autopct='%1.1f%%', colors=['lightgreen', 'lightcoral'])\n",
    "axes[0, 0].set_title('Distribution of Answerable vs Unanswerable Questions')\n",
    "\n",
    "# 2. Question length distribution\n",
    "question_lengths = [len(q.split()) for q in sample_dataset['question']]\n",
    "axes[0, 1].hist(question_lengths, bins=8, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].set_title('Question Length Distribution')\n",
    "axes[0, 1].set_xlabel('Number of Words')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Context length distribution\n",
    "context_lengths = [len(c.split()) for c in sample_dataset['context']]\n",
    "axes[1, 0].hist(context_lengths, bins=8, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1, 0].set_title('Context Length Distribution')\n",
    "axes[1, 0].set_xlabel('Number of Words')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 4. Relevance score distribution\n",
    "relevance_scores = sample_dataset['relevance_score']\n",
    "answerable_scores = [score for score, ans in zip(relevance_scores, sample_dataset['is_answerable']) if ans]\n",
    "unanswerable_scores = [score for score, ans in zip(relevance_scores, sample_dataset['is_answerable']) if not ans]\n",
    "\n",
    "axes[1, 1].hist(answerable_scores, bins=6, alpha=0.7, label='Answerable', color='lightgreen', edgecolor='black')\n",
    "axes[1, 1].hist(unanswerable_scores, bins=6, alpha=0.7, label='Unanswerable', color='lightcoral', edgecolor='black')\n",
    "axes[1, 1].set_title('Relevance Score Distribution')\n",
    "axes[1, 1].set_xlabel('Relevance Score')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing Exploration\n",
    "\n",
    "Let's explore how the data preprocessing pipeline transforms our raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor(config)\n",
    "\n",
    "print(\"Preprocessor Configuration:\")\n",
    "print(f\"- Tokenizer: {preprocessor.tokenizer.__class__.__name__}\")\n",
    "print(f\"- Max sequence length: {preprocessor.max_seq_length}\")\n",
    "print(f\"- Passage max length: {preprocessor.passage_max_length}\")\n",
    "print(f\"- Answer max length: {preprocessor.answer_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore confidence feature extraction\n",
    "example = sample_dataset[0]\n",
    "features_example = preprocessor._extract_confidence_features(example)\n",
    "\n",
    "print(\"Confidence Features for Example:\")\n",
    "print(f\"Question: {example['question']}\")\n",
    "print(f\"Context: {example['context']}\")\n",
    "print(\"\\nExtracted Features:\")\n",
    "features = features_example['confidence_features']\n",
    "for key, value in features.items():\n",
    "    print(f\"  {key}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confidence features across all examples\n",
    "all_features = []\n",
    "for i in range(len(sample_dataset)):\n",
    "    example = sample_dataset[i]\n",
    "    features_example = preprocessor._extract_confidence_features(example)\n",
    "    features = features_example['confidence_features']\n",
    "    features['is_answerable'] = example['is_answerable']\n",
    "    all_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "print(\"Confidence Features Summary:\")\n",
    "print(features_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature differences between answerable and unanswerable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "feature_cols = ['question_length', 'context_length', 'question_word_overlap', \n",
    "                'has_question_words', 'context_complexity', 'retrieval_score']\n",
    "\n",
    "for i, feature in enumerate(feature_cols):\n",
    "    answerable_values = features_df[features_df['is_answerable'] == True][feature]\n",
    "    unanswerable_values = features_df[features_df['is_answerable'] == False][feature]\n",
    "    \n",
    "    axes[i].hist(answerable_values, bins=5, alpha=0.7, label='Answerable', \n",
    "                color='lightgreen', edgecolor='black')\n",
    "    axes[i].hist(unanswerable_values, bins=5, alpha=0.7, label='Unanswerable', \n",
    "                color='lightcoral', edgecolor='black')\n",
    "    axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture Exploration\n",
    "\n",
    "Let's explore the model architecture and understand how the confidence calibration works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AdaptiveRetrievalQAModel(config)\n",
    "\n",
    "print(\"Model Architecture Summary:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "qa_params = sum(p.numel() for p in model.qa_model.parameters())\n",
    "calibrator_params = sum(p.numel() for p in model.calibrator.parameters())\n",
    "\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\"QA Model Parameters: {qa_params:,}\")\n",
    "print(f\"Calibrator Parameters: {calibrator_params:,}\")\n",
    "print(f\"Calibrator Ratio: {calibrator_params/total_params:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore model components\n",
    "print(\"QA Model:\")\n",
    "print(f\"  Type: {model.qa_model.__class__.__name__}\")\n",
    "print(f\"  Hidden size: {model.qa_model.config.hidden_size}\")\n",
    "print(f\"  Num layers: {model.qa_model.config.num_hidden_layers}\")\n",
    "\n",
    "print(\"\\nRetriever:\")\n",
    "print(f\"  Type: {model.retriever.__class__.__name__}\")\n",
    "print(f\"  Embedding dimension: {model.retriever.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "print(\"\\nCalibrator:\")\n",
    "print(f\"  Input dim: {model.calibrator.qa_hidden_size + model.calibrator.confidence_features_dim + 2}\")\n",
    "print(f\"  Temperature: {model.calibrator.temperature.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess sample dataset for model input\n",
    "print(\"Preprocessing sample dataset...\")\n",
    "processed_dataset = preprocessor.preprocess_dataset(sample_dataset, is_training=True)\n",
    "\n",
    "print(f\"Processed dataset columns: {processed_dataset.column_names}\")\n",
    "print(f\"Example input IDs shape: {len(processed_dataset[0]['input_ids'])}\")\n",
    "print(f\"Example passage embedding shape: {len(processed_dataset[0]['passage_embeddings'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Forward Pass Analysis\n",
    "\n",
    "Let's analyze how the model processes inputs and generates predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small batch for testing\n",
    "batch_size = 4\n",
    "batch_examples = processed_dataset.select(range(batch_size))\n",
    "\n",
    "# Convert to tensors\n",
    "batch = {\n",
    "    'input_ids': torch.tensor([ex['input_ids'] for ex in batch_examples]),\n",
    "    'attention_mask': torch.tensor([ex['attention_mask'] for ex in batch_examples]),\n",
    "    'is_answerable': torch.tensor([ex['is_answerable'] for ex in batch_examples], dtype=torch.float),\n",
    "    'retrieval_scores': torch.tensor([0.8, 0.6, 0.9, 0.3]),  # Example scores\n",
    "    'confidence_features': torch.randn(batch_size, 10)  # Example features\n",
    "}\n",
    "\n",
    "if 'start_positions' in batch_examples[0]:\n",
    "    batch['start_positions'] = torch.tensor([ex['start_positions'] for ex in batch_examples])\n",
    "    batch['end_positions'] = torch.tensor([ex['end_positions'] for ex in batch_examples])\n",
    "\n",
    "print(f\"Batch shapes:\")\n",
    "for key, value in batch.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "\n",
    "print(\"Model Outputs:\")\n",
    "for key, value in outputs.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: {value.shape} | range: [{value.min():.3f}, {value.max():.3f}]\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze answerability predictions\n",
    "answerability_probs = outputs['answerability_probs'].numpy()\n",
    "true_labels = batch['is_answerable'].numpy()\n",
    "\n",
    "print(\"Answerability Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "for i in range(batch_size):\n",
    "    question = sample_dataset[i]['question']\n",
    "    pred_prob = answerability_probs[i]\n",
    "    true_label = bool(true_labels[i])\n",
    "    predicted = pred_prob > config.get('model.confidence_threshold', 0.5)\n",
    "    \n",
    "    status = \"✓\" if predicted == true_label else \"✗\"\n",
    "    print(f\"Q{i+1}: {question[:50]}...\")\n",
    "    print(f\"     True: {true_label} | Pred: {predicted} ({pred_prob:.3f}) {status}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Retrieval System Analysis\n",
    "\n",
    "Let's explore the retrieval component and see how it finds relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build passage index\n",
    "passages = [\n",
    "    \"Paris is the capital and largest city of France, located in north-central France.\",\n",
    "    \"London is the capital and largest city of England and the United Kingdom.\",\n",
    "    \"Berlin is the capital and largest city of Germany.\",\n",
    "    \"Tokyo is the capital of Japan and the most populous metropolitan area in the world.\",\n",
    "    \"Rome is the capital city of Italy and a special comune.\",\n",
    "    \"Madrid is the capital and most-populous city of Spain.\",\n",
    "    \"The telephone was invented by Alexander Graham Bell in 1876.\",\n",
    "    \"The speed of light in vacuum is exactly 299,792,458 metres per second.\",\n",
    "    \"World War II ended on September 2, 1945, when Japan formally surrendered.\",\n",
    "    \"Python programming language was created by Guido van Rossum in the late 1980s.\"\n",
    "]\n",
    "\n",
    "print(f\"Building passage index with {len(passages)} passages...\")\n",
    "model.build_passage_index(passages)\n",
    "print(\"Passage index built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval for different queries\n",
    "test_queries = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who invented the telephone?\",\n",
    "    \"When did World War II end?\",\n",
    "    \"What is the meaning of life?\",  # Should retrieve less relevant passages\n",
    "]\n",
    "\n",
    "print(\"Retrieval Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    retrieved_passages, scores = model.retrieve_passages(query, top_k=3)\n",
    "    \n",
    "    for i, (passage, score) in enumerate(zip(retrieved_passages, scores)):\n",
    "        print(f\"  {i+1}. ({score:.3f}) {passage[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze retrieval score distribution\n",
    "all_scores = []\n",
    "query_types = []\n",
    "\n",
    "for query in test_queries:\n",
    "    _, scores = model.retrieve_passages(query, top_k=5)\n",
    "    all_scores.extend(scores)\n",
    "    query_types.extend([query[:20] + '...'] * len(scores))\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "retrieval_df = pd.DataFrame({\n",
    "    'score': all_scores,\n",
    "    'query': query_types\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=retrieval_df, x='query', y='score')\n",
    "plt.title('Retrieval Score Distribution by Query Type')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. End-to-End Prediction Analysis\n",
    "\n",
    "Let's test the complete system with end-to-end predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test end-to-end predictions\n",
    "test_questions = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Who invented the telephone?\", \n",
    "    \"What is the speed of light?\",\n",
    "    \"What is the meaning of life?\",  # Should be marked as unanswerable\n",
    "    \"How do you bake a cake?\",       # No relevant passage\n",
    "]\n",
    "\n",
    "print(\"End-to-End Prediction Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "predictions = []\n",
    "for question in test_questions:\n",
    "    result = model.predict(question, return_confidence=True)\n",
    "    predictions.append(result)\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Answer: '{result['answer']}'\")\n",
    "    print(f\"Is Answerable: {result['is_answerable']} (confidence: {result['confidence']:.3f})\")\n",
    "    print(f\"QA Confidence: {result['qa_confidence']:.3f}\")\n",
    "    print(f\"Top Retrieved: {result['retrieved_passages'][0][:60]}...\")\n",
    "    print(f\"Retrieval Score: {result['retrieval_scores'][0]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction confidence vs retrieval scores\n",
    "answerability_confs = [p['confidence'] for p in predictions]\n",
    "qa_confs = [p['qa_confidence'] for p in predictions]\n",
    "retrieval_scores = [p['retrieval_scores'][0] for p in predictions]\n",
    "is_answerable = [p['is_answerable'] for p in predictions]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# 1. Answerability confidence vs retrieval score\n",
    "colors = ['green' if ans else 'red' for ans in is_answerable]\n",
    "axes[0].scatter(retrieval_scores, answerability_confs, c=colors, s=100, alpha=0.7)\n",
    "axes[0].axhline(y=config.get('model.confidence_threshold', 0.5), \n",
    "                color='black', linestyle='--', label='Decision Threshold')\n",
    "axes[0].set_xlabel('Retrieval Score')\n",
    "axes[0].set_ylabel('Answerability Confidence')\n",
    "axes[0].set_title('Answerability Confidence vs Retrieval Score')\n",
    "axes[0].legend(['Threshold', 'Answerable', 'Unanswerable'])\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. QA confidence vs answerability confidence\n",
    "axes[1].scatter(qa_confs, answerability_confs, c=colors, s=100, alpha=0.7)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect Correlation')\n",
    "axes[1].set_xlabel('QA Confidence')\n",
    "axes[1].set_ylabel('Answerability Confidence')\n",
    "axes[1].set_title('QA Confidence vs Answerability Confidence')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation Metrics Analysis\n",
    "\n",
    "Let's explore the evaluation metrics and understand how they measure system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = AnswerabilityCalibrationMetrics(config)\n",
    "\n",
    "# Create dummy predictions for metric analysis\n",
    "n_samples = 100\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate realistic predictions\n",
    "true_labels = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])  # 70% answerable\n",
    "predicted_probs = np.where(\n",
    "    true_labels == 1,\n",
    "    np.random.beta(3, 1, n_samples),  # Higher probs for answerable\n",
    "    np.random.beta(1, 3, n_samples)   # Lower probs for unanswerable\n",
    ")\n",
    "\n",
    "print(f\"Generated {n_samples} synthetic predictions for metric analysis\")\n",
    "print(f\"True distribution: {np.mean(true_labels):.1%} answerable\")\n",
    "print(f\"Average predicted probability: {np.mean(predicted_probs):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate calibration metrics\n",
    "ece = evaluator._calculate_ece(predicted_probs, true_labels)\n",
    "mce = evaluator._calculate_mce(predicted_probs, true_labels)\n",
    "\n",
    "print(f\"Calibration Metrics:\")\n",
    "print(f\"Expected Calibration Error (ECE): {ece:.4f}\")\n",
    "print(f\"Maximum Calibration Error (MCE): {mce:.4f}\")\n",
    "\n",
    "# Calculate other metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "\n",
    "threshold = config.get('model.confidence_threshold', 0.5)\n",
    "predicted_labels = (predicted_probs > threshold).astype(int)\n",
    "\n",
    "auroc = roc_auc_score(true_labels, predicted_probs)\n",
    "avg_precision = average_precision_score(true_labels, predicted_probs)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "print(f\"\\nClassification Metrics:\")\n",
    "print(f\"AUROC: {auroc:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create calibration curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "fraction_of_positives, mean_predicted_value = calibration_curve(\n",
    "    true_labels, predicted_probs, n_bins=10\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Calibration curve\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "plt.plot(mean_predicted_value, fraction_of_positives, 'o-', label='Model')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title(f'Calibration Curve (ECE = {ece:.3f})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence histogram\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(predicted_probs[true_labels == 0], bins=15, alpha=0.5, \n",
    "         label='Unanswerable', color='red', density=True)\n",
    "plt.hist(predicted_probs[true_labels == 1], bins=15, alpha=0.5, \n",
    "         label='Answerable', color='green', density=True)\n",
    "plt.axvline(threshold, color='black', linestyle='--', label=f'Threshold ({threshold})')\n",
    "plt.xlabel('Predicted Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Confidence Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Findings\n",
    "\n",
    "Let's summarize the key insights from our exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"KEY INSIGHTS FROM EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. DATA CHARACTERISTICS:\")\n",
    "print(f\"   • Sample dataset has {stats['answerable_ratio']:.1%} answerable questions\")\n",
    "print(f\"   • Average question length: {stats['avg_question_length']:.1f} words\")\n",
    "print(f\"   • Average context length: {stats['avg_context_length']:.1f} words\")\n",
    "print(f\"   • Unanswerable questions tend to have lower relevance scores\")\n",
    "\n",
    "print(\"\\n2. MODEL ARCHITECTURE:\")\n",
    "print(f\"   • Total parameters: {total_params:,}\")\n",
    "print(f\"   • Calibrator represents {calibrator_params/total_params:.1%} of total parameters\")\n",
    "print(f\"   • Combines QA confidence, retrieval scores, and question features\")\n",
    "print(f\"   • Uses temperature scaling for confidence calibration\")\n",
    "\n",
    "print(\"\\n3. RETRIEVAL SYSTEM:\")\n",
    "print(f\"   • Successfully retrieves relevant passages for factual questions\")\n",
    "print(f\"   • Retrieval scores correlate with question answerability\")\n",
    "print(f\"   • Lower scores for abstract questions (meaning of life, etc.)\")\n",
    "\n",
    "print(\"\\n4. CONFIDENCE CALIBRATION:\")\n",
    "print(f\"   • ECE on synthetic data: {ece:.4f} (lower is better)\")\n",
    "print(f\"   • System learns to correlate multiple confidence signals\")\n",
    "print(f\"   • Threshold-based decision making at {threshold} confidence\")\n",
    "\n",
    "print(\"\\n5. NOVEL CONTRIBUTIONS:\")\n",
    "print(\"   • Joint modeling of retrieval and QA confidence\")\n",
    "print(\"   • Multi-signal confidence calibration approach\")\n",
    "print(\"   • Addresses LLM hallucination in retrieval-augmented systems\")\n",
    "print(\"   • Production-ready answerability prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Future Research Directions\n",
    "\n",
    "Based on our exploration, here are promising research directions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FUTURE RESEARCH DIRECTIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. IMPROVED CALIBRATION METHODS:\")\n",
    "print(\"   • Investigate Platt scaling vs isotonic regression\")\n",
    "print(\"   • Multi-class calibration for confidence levels\")\n",
    "print(\"   • Dynamic temperature adaptation during inference\")\n",
    "\n",
    "print(\"\\n2. ADVANCED RETRIEVAL STRATEGIES:\")\n",
    "print(\"   • Dense-sparse hybrid retrieval\")\n",
    "print(\"   • Multi-hop reasoning for complex questions\")\n",
    "print(\"   • Query expansion and reformulation\")\n",
    "\n",
    "print(\"\\n3. FEATURE ENGINEERING:\")\n",
    "print(\"   • Semantic question type classification\")\n",
    "print(\"   • Answer type prediction (factual, opinion, etc.)\")\n",
    "print(\"   • Contextual complexity measures\")\n",
    "\n",
    "print(\"\\n4. EVALUATION IMPROVEMENTS:\")\n",
    "print(\"   • Domain-specific evaluation metrics\")\n",
    "print(\"   • Human evaluation of answerability decisions\")\n",
    "print(\"   • Cost-aware evaluation (false positive vs false negative costs)\")\n",
    "\n",
    "print(\"\\n5. PRODUCTION CONSIDERATIONS:\")\n",
    "print(\"   • Real-time inference optimization\")\n",
    "print(\"   • Uncertainty quantification\")\n",
    "print(\"   • User feedback incorporation\")\n",
    "print(\"   • A/B testing frameworks for confidence thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exploration has demonstrated the key components and novel approach of the Adaptive Retrieval QA system:\n",
    "\n",
    "1. **Novel Architecture**: The system successfully combines retrieval, question answering, and confidence calibration in a unified framework.\n",
    "\n",
    "2. **Multi-Signal Confidence**: By jointly modeling retrieval relevance, QA model confidence, and question characteristics, the system makes more informed answerability decisions.\n",
    "\n",
    "3. **Calibration Focus**: The confidence calibration module addresses a critical gap in production AI systems - knowing when to abstain from answering.\n",
    "\n",
    "4. **Practical Impact**: This approach directly addresses LLM hallucination problems in retrieval-augmented generation systems.\n",
    "\n",
    "The exploration validates the core hypothesis that joint modeling of multiple confidence signals leads to better answerability prediction and more reliable AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}